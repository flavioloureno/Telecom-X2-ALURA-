# -*- coding: utf-8 -*-
"""Challenge_TelecomX_2_BR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-xxqT_xG0QJyqIebTQcJdSPayh37Nu3j

<center>
<b><font size="10">Telecom X - Parte 2</font></b>
</center>

<br/>
<center>
<b><font color="blue" size="6">Prevendo Cancelamento</font></b>
</center>

üéØ <b><font color="blue">MISS√ÉO:</font></b>

 - Desenvolver **modelos preditivos** capazes de prever quais clientes t√™m maior chance de cancelar seus servi√ßos.

 - A empresa quer **antecipar o problema da evas√£o**, assim ser√° constru√≠do um pipeline robusto para essa etapa inicial de modelagem.

üß† <b><font color="blue">OBJETIVO DO DESAFIO:</font></b>

 - Preparar os dados para a **modelagem** (*tratamento, encoding, normaliza√ß√£o*).

 - Realizar **an√°lise de correla√ß√£o** e sele√ß√£o de vari√°veis.

 - Treinar dois ou mais modelos de classifica√ß√£o.

 - Avaliar o desempenho dos modelos com **m√©tricas**.

 - **Interpretar os resultados**, incluindo a import√¢ncia das vari√°veis.

 - Criar uma **conclus√£o estrat√©gica** apontando os principais fatores que influenciam a evas√£o.

#üìå <b><font color="blue" size="6">EXTRA√á√ÉO</font></b>


---

Para iniciar an√°lise, importado os [dados da API da Telecom X](https://drive.google.com/file/d/1Dul9YDaHCzXo8jFJKceEhzarRkjwLYnK/view?usp=drive_link). Esses dados est√£o dispon√≠veis no formato `CSV` e cont√™m as informa√ß√µes j√° tratadas.
"""

import pandas as pd
import requests
import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import gdown
from IPython.display import display, HTML

# ‚úÖ ID do arquivo no Google Drive
file_id = '1Dul9YDaHCzXo8jFJKceEhzarRkjwLYnK'
url = f'https://drive.google.com/uc?id={file_id}'

# ‚¨áÔ∏è Baixar o arquivo
gdown.download(url, 'dados_tratados.csv', quiet=False)

# üìñ Ler os dados
dados = pd.read_csv('dados_tratados.csv')
dados.head()

"""#üîß <b><font color="blue" size="6">PREPARA√á√ÉO DOS DADOS</font></b>


---

## üéØ<b><font color="blue">Remo√ß√£o de Colunas Irrelevantes</font></b>

Removido coluna com IDs, estimativas e valores duplicados em outras colunas. Essas colunas n√£o ajudam na previs√£o da evas√£o e podem at√© prejudicar o desempenho dos modelos.
"""

# Calcular correla√ß√£o apenas entre colunas num√©ricas
dados_numericos = dados.select_dtypes(include=['number', 'bool'])
correlacao = dados_numericos.corr().abs()

# Pegar pares com alta correla√ß√£o
pares_correlacionados = [
    (col1, col2)
    for col1 in correlacao.columns
    for col2 in correlacao.columns
    if col1 != col2 and correlacao.loc[col1, col2] > 0.95
]

print("üîÅ PARES DE COLUNAS COM ALTA CORRELA√á√ÉO:")
print('-'*41)
for col1, col2 in pares_correlacionados:
    print(f"{col1}  üîõ  {col2}")

# Remover as colunas com alta correla√ß√£o
dados = dados.drop(['ID_Cliente', 'tempo_estimado_meses', 'Conta_Diarias', 'Valor_Total'], axis=1)

# Remover registros onde o valor √© 'N√£o informado'
dados = dados[dados['Cancelamento'] != 'N√£o informado']


dados.head()

"""## üéØ <b><font color="blue">Encoding</font></b>

Inicialmente foi identificado os valores √∫nicos de cada coluna e posteriormente transformado as vari√°veis categ√≥ricas em formato num√©rico para torn√°-las compat√≠veis com algoritmos de machine learning. Utilizou-se um m√©todo de codifica√ß√£o adequado, como o **one-hot encoding**.
"""

# Para saber os valores √∫nicos por coluna e quantidade.
for col in dados.select_dtypes(include='object').columns:
    titulo = f"<strong style='font-size: 15px;'>‚û°Ô∏è  {col.upper()}</strong>"
    display(HTML(titulo))
    display(dados[col].value_counts(dropna=False).head(3).to_frame(name='Frequ√™ncia'))
    print("-" * 35)

# Passo 1: Mapear os valores para bin√°rio
import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=FutureWarning)
    dados = dados.replace({'Yes': 1, 'No': 0})
    dados = dados.replace({'Female': 1, 'Male': 0})
    dados = dados.replace({'True': 1, 'False': 0})

dados.info()

# Passo 2: Identificar colunas que t√™m s√≥ 0 e 1
binarias = [col for col in dados.columns if set(dados[col].dropna().unique()).issubset({0, 1})]

# Passo 3: Converter essas colunas para booleano
dados[binarias] = dados[binarias].astype(bool)

dados.info()

# Passo 4: One-hot encoding para todas as colunas categ√≥ricas multiclasse
dados = pd.get_dummies(dados, drop_first=True)

dados.info()

# Passo 5: Calcular novamente a correla√ß√£o apenas entre colunas num√©ricas
dados_numericos = dados.select_dtypes(include=['number', 'bool'])
correlacao = dados_numericos.corr().abs()

# Pegar pares com alta correla√ß√£o
pares_correlacionados = [
    (col1, col2)
    for col1 in correlacao.columns
    for col2 in correlacao.columns
    if col1 != col2 and correlacao.loc[col1, col2] > 0.95
]

print("üîÅ PARES DE COLUNAS COM ALTA CORRELA√á√ÉO:")
print('-'*40)
for col1, col2 in pares_correlacionados:
    print(f"{col1}  üîõ  {col2}")

# Passo 6: Remover colunas com alta correla√ß√£o:
dados = dados.drop([ 'Multiplas_Linhas_No phone service',
    'Backup_Online_No internet service',
    'Protecao_Dispositivo_No internet service',
    'Suporte_Tecnico_No internet service',
    'Streaming_TV_No internet service',
    'Streaming_Filmes_No internet service'], axis=1)

# Remover registros onde o valor √© 'N√£o informado'
dados = dados[dados['Cancelamento'] != 'N√£o informado']


dados.head()

"""## üéØ <b><font color="blue">Verifica√ß√£o da Propor√ß√£o de Evas√£o</font></b>

Calculamos a propor√ß√£o de clientes cancelados (evadidos) e os que permaneceram. A distribui√ß√£o ser√° usada para definir se o dataset est√° balanceado.
"""

# Frequ√™ncia absoluta e relativa da coluna Cancelamento
frequencia = dados['Cancelamento'].value_counts()
proporcao = dados['Cancelamento'].value_counts(normalize=True) * 100

# Visualizar com gr√°fico de barras
plt.figure(figsize=(10, 6))
sns.barplot(x=proporcao.index, y=proporcao.values, hue=proporcao.index, palette='coolwarm', legend=False)
plt.ylabel('Propor√ß√£o (%)')
plt.title('Distribui√ß√£o da Evas√£o de Clientes')
plt.show()

# Exibir os resultados
print("\nüìä FREQUENCIA ABSOLUTA:")
print("-" * 23)
print(frequencia)
print("-\n" * 1)
print("üìà PROPOR√á√ÉO (%):")
print("-" * 17)
print(proporcao)
print("-\n")

"""###üìå Ap√≥s rodar seu c√≥digo, veja:


 - **Equil√≠brio**: Quando as classes est√£o pr√≥ximas de 50% / 50%.

 - **Gerenci√°vel quando pr√≥ximo de 70% / 30%** .Se 70% dos clientes permaneceram e 30% cancelaram, ainda √© poss√≠vel treinar bons modelos com t√©cnicas de balanceamento

 - **Desequil√≠brio forte**: Quando uma classe tem 80% ou mais dos registros (ex: 80% "N√£o" e 20% "Sim"), o modelo pode ignorar a classe minorit√°ria, e ser√° importante:

  - Aplicar t√©cnicas de balanceamento (como oversampling com SMOTE, undersampling, etc.);

  - Escolher m√©tricas apropriadas (como f1-score, recall, AUC) ‚Äî e n√£o apenas acur√°cia.

## üéØ <b><font color="blue">Balanceamento de Classes</font></b>

Para aprofundar a an√°lise, foi aplicado t√©cnicas de balanceamento como **undersampling** , **oversampling** e **SMOTE**. Em situa√ß√µes de forte desbalanceamento, ferramentas como o **SMOTE** podem ser √∫teis para gerar exemplos sint√©ticos da classe minorit√°ria.
"""

# Separar features(X) e target(y)
X = dados.drop('Cancelamento', axis=1)
y = dados['Cancelamento']

# Oversampling com RandomOverSampler (Duplica registros da classe minorit√°ria)

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_res, y_res = ros.fit_resample(X, y)

print("DISTRIBUI√á√ÉO AP√ìS OVERSAMPLING:")
print("-" * 31)
print(pd.Series(y_res).value_counts())

# Undersampling com RandomUnderSampler (Remove registros da classe majorit√°ria)
# Gera novas amostras sint√©ticas da classe minorit√°ria, em vez de apenas copiar.

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_res, y_res = rus.fit_resample(X, y)

print("DISTRIBUI√á√ÉO AP√ìS UNDERSAMPLING:")
print("-" * 32)
print(pd.Series(y_res).value_counts())

# SMOTE (Synthetic Minority Over-sampling Technique)
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

print("DISTRIBUI√á√ÉO AP√ìS SMOTE:")
print("-" * 24)
print(pd.Series(y_res).value_counts())

"""### üß™ **Balanceamento de Classes**

Antes da cria√ß√£o dos modelos, foi analisada a propor√ß√£o entre as classes da vari√°vel-alvo `Cancelamento`, que indica se o cliente **evadiu** (1) ou **permaneceu** (0). Identificou-se um desequil√≠brio entre as classes, o que poderia comprometer o desempenho preditivo dos algoritmos, especialmente em rela√ß√£o √† detec√ß√£o de clientes propensos √† evas√£o.

Para lidar com esse problema, foram testadas tr√™s t√©cnicas de balanceamento:

- **Oversampling com `RandomOverSampler`**: Aumenta a quantidade da classe minorit√°ria replicando seus exemplos.
- **Undersampling com `RandomUnderSampler`**: Reduz a quantidade da classe majorit√°ria removendo exemplos.
- **`SMOTE` (Synthetic Minority Over-sampling Technique)**: Gera exemplos sint√©ticos da classe minorit√°ria com base nos vizinhos mais pr√≥ximos.

Ap√≥s a compara√ß√£o das distribui√ß√µes, optou-se por seguir com a t√©cnica **SMOTE**, pois ela mant√©m todos os dados originais e cria novos registros de forma artificial, evitando duplica√ß√µes e reduzindo o risco de overfitting.

Os dados balanceados foram ent√£o utilizados para treinar e avaliar os modelos preditivos. O desempenho foi posteriormente analisado por meio de m√©tricas como **precis√£o**, **recall**, **f1-score** e **matriz de confus√£o**, que ser√£o apresentados nas pr√≥ximas etapas.

## üéØ  <b><font color="blue">Normaliza√ß√£o ou Padroniza√ß√£o </font></b>

Foi avaliado a necessidade de normalizar ou padronizar os dados, conforme os modelos que ser√£o aplicados.
Modelos baseados em dist√¢ncia, como KNN, SVM, Regress√£o Log√≠stica e Redes Neurais, requerem esse pr√©-processamento.
J√° modelos baseados em √°rvore, como Decision Tree, Random Forest e XGBoost, n√£o s√£o sens√≠veis √† escala dos dados.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Separar X e y
X = dados.drop(columns=['Cancelamento'])
y = dados['Cancelamento']

# Dividir treino e teste (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Identificar colunas num√©ricas para padronizar
colunas_numericas = ['Meses_de_Contrato', 'Valor_Mensal']

# Criar c√≥pias para n√£o modificar original
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

# Aplicar StandardScaler s√≥ nas colunas num√©ricas
scaler = StandardScaler()
X_train_scaled[colunas_numericas] = scaler.fit_transform(X_train[colunas_numericas])
X_test_scaled[colunas_numericas] = scaler.transform(X_test[colunas_numericas])

# Agora se pode usar X_train_scaled e X_test_scaled para treinar modelos sens√≠veis √† escala.

"""### ‚öôÔ∏è **Normaliza√ß√£o / Padroniza√ß√£o dos Dados**

Como parte do pr√©-processamento, foi aplicada a **padroniza√ß√£o** dos atributos num√©ricos, utilizando a t√©cnica `StandardScaler`, que transforma os dados para que tenham m√©dia zero e desvio padr√£o um.

Esse passo √© fundamental para o bom desempenho de algoritmos que s√£o sens√≠veis √† escala dos dados, como:

- **KNN** (K-Nearest Neighbors)
- **SVM** (Support Vector Machines)
- **Regress√£o Log√≠stica**
- **Redes Neurais**

A padroniza√ß√£o foi aplicada **apenas ap√≥s o balanceamento das classes e a separa√ß√£o entre dados de treino e teste**, para evitar vazamento de dados e garantir que as transforma√ß√µes ocorram apenas com base nos dados de treino. Com isso, garantimos uma compara√ß√£o justa e v√°lida durante a avalia√ß√£o dos modelos.

# üìä <b><font color="blue" size="6">CORRELA√á√ÉO E SELE√á√ÉO DE VARI√ÅVEIS</font></b>

## üéØ <b><font color="green">An√°lise de Correla√ß√£o</font></b>

A matriz de correla√ß√£o √© √∫til para identificar rela√ß√µes entre vari√°veis num√©ricas. Observe especialmente quais vari√°veis apresentam maior correla√ß√£o com a evas√£o, pois elas podem ser fortes candidatas para o modelo preditivo.
"""

# Identificar as colunas num√©ricas automaticamente
colunas_numericas = dados.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Para garantir que a coluna alvo 'Cancelamento' esteja inclu√≠da
if 'Cancelamento' not in colunas_numericas:
    colunas_numericas.append('Cancelamento')

# Criar um DataFrame apenas com as colunas num√©ricas
dados_numericos = dados[colunas_numericas]


# Calcular matriz de correla√ß√£o
corr = dados_numericos.corr()

# Visualizar matriz de correla√ß√£o
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title('Matriz de Correla√ß√£o das Vari√°veis Num√©ricas')
plt.show()


# Analisar correla√ß√£o com evas√£o ---
corr_com_alvo = corr['Cancelamento'].drop('Cancelamento').sort_values(ascending=False)

"""### üîç **An√°lise de Correla√ß√£o**

Foi utilizada a matriz de correla√ß√£o para identificar poss√≠veis rela√ß√µes entre as vari√°veis num√©ricas e a vari√°vel-alvo `Cancelamento`. A an√°lise permite identificar vari√°veis que apresentam maior influ√™ncia na evas√£o dos clientes.

A matriz de correla√ß√£o retornou as vari√°veis `Meses_de_Contrato` e `Valor_Mensal`, pois s√£o as √∫nicas vari√°veis num√©ricas cont√≠nuas do conjunto de dados. As demais vari√°veis s√£o majoritariamente bin√°rias (indicadoras), o que limita sua visualiza√ß√£o na matriz padr√£o.

Para complementar, foi plotado um gr√°fico com as vari√°veis mais correlacionadas com `Cancelamento`, com o objetivo de enriquecer a an√°lise e identificar fatores relevantes para a evas√£o:

 - A **correla√ß√£o entre ‚ÄúMeses_de_Contrato‚Äù e ‚ÄúValor_Mensal‚Äù  √© fraca (0.25)**.

 - A **correla√ß√£o entre ‚ÄúMeses_de_Contrato‚Äù e ‚ÄúCancelamento‚Äù  √© fraca (0.35)**.

 - A **correla√ß√£o entre ‚ÄúValor_Mensal‚Äù e ‚ÄúCancelamento‚Äù  √© fraca (0.19)**.

üîé *Isso sugere que n√£o h√° uma rela√ß√£o linear forte entre o tempo de contrato e o valor pago mensalmente, e entre cada uma das vari√°veis com o cancelamento.*

## üéØ <b><font color="green">An√°lise Direcionadas</font></b>

Nesta etapa, analisamos como duas vari√°veis principais ‚Äî **Meses de Contrato** e **Valor Mensal** ‚Äî se comportam em rela√ß√£o ao **Cancelamento (Evas√£o)** dos clientes.
"""

fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 1 linha, 2 colunas

# Boxplot com hue e legend=False para evitar warning
sns.boxplot(ax=axes[0], x='Cancelamento', y='Meses_de_Contrato', hue='Cancelamento', palette='coolwarm', data=dados, legend=False)
axes[0].set_title('Tempo de Contrato por Evas√£o',fontsize=12)
axes[0].set_ylabel('Meses de Contrato')

sns.stripplot(ax=axes[1], x='Cancelamento', y='Valor_Mensal', hue='Cancelamento',jitter=True, alpha=0.5, data=dados, legend=False)
axes[1].set_title('Valor Mensal por Evas√£o', fontsize=12)
axes[1].set_ylabel('Valor Mensal')

plt.tight_layout()

# Aumenta o espa√ßo horizontal entre os gr√°ficos
plt.subplots_adjust(hspace=0.5, wspace=0.3)
plt.show()

"""### üéØ **An√°lise Direcionada**



---

#### üìä **Gr√°fico 1**: *Boxplot - Tempo de Contrato por Evas√£o*

O **boxplot** apresenta a distribui√ß√£o dos **meses de contrato** para clientes que **permaneceram (0)** e os que **evadiram (1)**.

- A mediana (linha central da caixa) mostra a tend√™ncia central de cada grupo.
- As caixas representam o intervalo interquartil (50% central dos dados).
- Pontos fora da caixa (outliers) indicam clientes com tempos de contrato muito diferentes da maioria.

‚á£‚á£‚á£

**üßê Interpreta√ß√£o:**  
Podemos observar que clientes que evadiram tendem a ter um **tempo de contrato menor** do que os que permaneceram. Isso pode indicar que a evas√£o ocorre mais frequentemente nos primeiros meses de contrato.

---

#### üéØ **Gr√°fico 2**: *Stripplot - Valor Mensal por Evas√£o*

O **stripplot** (tamb√©m chamado de *scatter plot categ√≥rico*) mostra **cada cliente individualmente**, posicionando seus **valores mensais** em rela√ß√£o √† evas√£o.

- Cada ponto representa um cliente.
- Os pontos est√£o "espalhados" horizontalmente (via `jitter`) para facilitar a visualiza√ß√£o da densidade.

‚á£‚á£‚á£

**üßê Interpreta√ß√£o:**

Apesar de haver sobreposi√ß√£o, √© poss√≠vel notar uma **concentra√ß√£o maior de valores mensais mais altos entre os clientes que evadiram (1)**, sugerindo que **valores mais elevados podem estar relacionados √† evas√£o**.

---

Esses dois gr√°ficos ajudam a entender melhor **quais fatores est√£o associados √† decis√£o dos clientes em cancelar os servi√ßos.**

# ü§ñ <b><font color="blue" size="6">MODELAGEM PREDITIVA</font></b>

## üéØ <b><font color="blue">Separa√ß√£o de Dados</font></b>

Uma divis√£o comum √© 70% para treino e 30% para teste, ou 80/20, dependendo do tamanho da base de dados.
"""

# Dividir treino e teste (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

"""## üéØ <b><font color="blue">Cria√ß√£o de Modelos</font></b>

Foi criado 4 modelos diferentes para prever a evas√£o de clientes:

 - *Regress√£o Log√≠stica* e *KNN* que exige normaliza√ß√£o.

 - *√Årvore de Decis√£o* e *Random Forest* que n√£o exige normaliza√ß√£o.

"""

# MODELO 1: Regress√£o Log√≠stica (com normaliza√ß√£o)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Normaliza√ß√£o apenas para o Modelo 1
scaler = StandardScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)

# Treinar modelo com dados normalizados
modelo_log = LogisticRegression()
modelo_log.fit(X_train_norm, y_train)

# Previs√µes
y_pred_log = modelo_log.predict(X_test_norm)

# Avalia√ß√£o
print("üîç REGRESS√ÉO LOG√çSTICA")
print("-" * 23)
print(confusion_matrix(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))

# MODELO 2: Random Forest

from sklearn.ensemble import RandomForestClassifier

# Treinar modelo com dados originais
modelo_rf = RandomForestClassifier(random_state=42)
modelo_rf.fit(X_train, y_train)

# Previs√µes
y_pred_rf = modelo_rf.predict(X_test)

# Avalia√ß√£o
print("üå≥ RANDOM FOREST")
print("-" * 17)
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

# MODELO 3: KNN - K Vizinhos Mais Pr√≥ximos (com normaliza√ß√£o)
from sklearn.neighbors import KNeighborsClassifier

modelo_knn = KNeighborsClassifier(n_neighbors=5)
modelo_knn.fit(X_train_norm, y_train)
y_pred_knn = modelo_knn.predict(X_test_norm)

# Avalia√ß√£o
print("üîç KNN")
print("-" * 10)
print(confusion_matrix(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))

# MODELO 4: √Årvore de Decis√£o (sem normaliza√ß√£o)
from sklearn.tree import DecisionTreeClassifier

modelo_dt = DecisionTreeClassifier(random_state=42)
modelo_dt.fit(X_train, y_train)
y_pred_dt = modelo_dt.predict(X_test)

# Avalia√ß√£o
print("üå≥ √ÅRVORE DE DECIS√ÉO")
print("-" * 21)
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Fun√ß√£o para gerar m√©tricas + matriz de confus√£o como string
def gerar_metricas(nome, modelo, X_test, y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    cm_str = f"[[{cm[0][0]}, {cm[0][1]}], [{cm[1][0]}, {cm[1][1]}]]"

    return {
        'Modelo': nome,
        'Acur√°cia': accuracy_score(y_test, y_pred),
        'Precis√£o': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'Matriz de Confus√£o': cm_str
    }

# Coletar m√©tricas de todos os modelos
resultados = [
    gerar_metricas("Regress√£o Log√≠stica", modelo_log, X_test_norm, y_test, y_pred_log),
    gerar_metricas("Random Forest", modelo_rf, X_test, y_test, y_pred_rf),
    gerar_metricas("KNN", modelo_knn, X_test_norm, y_test, y_pred_knn),
    gerar_metricas("√Årvore de Decis√£o", modelo_dt, X_test, y_test, y_pred_dt)
]

# Criar DataFrame
df_resultados = pd.DataFrame(resultados)
df_resultados = df_resultados.sort_values(by='F1-Score', ascending=False)

# Exibir a tabela
print("üìä COMPARATIVO DE MODELOS")
print("-" * 26)
display(df_resultados.round(4))

# Criar DataFrame com os dados fornecidos
dados_modelos = pd.DataFrame({
    'Modelo': ['Regress√£o Log√≠stica', 'Random Forest', 'KNN', '√Årvore de Decis√£o'],
    'Acur√°cia': [0.7996, 0.7896, 0.7569, 0.7264],
    'Precis√£o': [0.6544, 0.6434, 0.5473, 0.4859],
    'Recall': [0.5214, 0.4679, 0.4947, 0.5053],
    'F1-Score': [0.5804, 0.5418, 0.5197, 0.4954]
})

# Plotar gr√°fico de barras para cada m√©trica
dados_modelos.set_index('Modelo').plot(kind='bar', figsize=(12, 6), colormap='Set3')
plt.title('Comparativo de Desempenho dos Modelos')
plt.ylabel('Pontua√ß√£o')
plt.ylim(0, 1)
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.legend(title='M√©tricas')
plt.tight_layout()
plt.show()

"""## ü§ñ  <b><font color="blue">Avalia√ß√£o dos Modelos </font></b>

Para prever a evas√£o de clientes, foram utilizados **quatro modelos diferentes**, com e sem normaliza√ß√£o. Cada modelo foi avaliado utilizando as seguintes m√©tricas:

- **Acur√°cia**  
- **Precis√£o**  
- **Recall**  
- **F1-score**  
- **Matriz de confus√£o**

<br/>

### üìå **MODELO 1**: *Regress√£o Log√≠stica* (com normaliza√ß√£o)

- Modelo linear, indicado para **classifica√ß√£o bin√°ria**.  
- Requer **normaliza√ß√£o dos dados**, pois √© sens√≠vel √† escala das vari√°veis.  
- F√°cil de interpretar e √∫til para identificar vari√°veis relevantes.  
- Serviu como **modelo base** e apresentou **o melhor desempenho geral** nas m√©tricas.

---

### üìå **MODELO 2**: *Random Forest* (sem normaliza√ß√£o)

- Modelo n√£o linear baseado em m√∫ltiplas √°rvores de decis√£o.  
- **N√£o precisa de normaliza√ß√£o**, pois suas decis√µes s√£o baseadas em cortes.  
- Robusto contra overfitting e apto a capturar rela√ß√µes complexas entre vari√°veis.  
- Teve desempenho s√≥lido, por√©m com menor recall em compara√ß√£o √† regress√£o log√≠stica.

---

### üìå **MODELO 3**: *KNN* ‚Äì K Vizinhos Mais Pr√≥ximos (com normaliza√ß√£o)

- Classifica os clientes com base na **dist√¢ncia dos vizinhos mais pr√≥ximos**.  
- Requer **normaliza√ß√£o**, pois √© altamente sens√≠vel √† escala das vari√°veis.  
- Simples e intuitivo, mas pode ter **performance inferior com dados desbalanceados**.  
- Apresentou **m√©tricas medianas** e sinal de **underfitting**.

---

### üìå **MODELO 4**: *√Årvore de Decis√£o* (sem normaliza√ß√£o)

- Modelo interpret√°vel baseado em regras do tipo "if-else".  
- **N√£o exige normaliza√ß√£o**.  
- Pode se ajustar bem aos dados, mas √© suscet√≠vel ao **overfitting** se n√£o regulado.  
- Foi o modelo com **menor desempenho geral**, indicando **underfitting**.

<br/>

### üèÜ <b><font color="blue">**CONCLUS√ÉO** </font></b>: Melhor Modelo √© **Regress√£o Log√≠stica**

Ap√≥s aplicar e comparar os quatro modelos (Regress√£o Log√≠stica, Random Forest, KNN e √Årvore de Decis√£o), considerando as m√©tricas de desempenho (Acur√°cia, Precis√£o, Recall, F1-Score e Matriz de Confus√£o), conclui-se:

- üîπ **Regress√£o Log√≠stica** apresentou o melhor desempenho geral:
  - Maior **acur√°cia (0.7996)** e **F1-Score (0.5804)**.
  - Bom equil√≠brio entre **precis√£o (0.6544)** e **recall (0.5214)**.
  - Modelo simples, interpret√°vel e eficiente, ideal para come√ßar.

- üîπ **Random Forest** teve desempenho pr√≥ximo, por√©m com recall mais baixo, o que pode impactar a detec√ß√£o de clientes que realmente evadiriam.

- üîπ **KNN** e **√Årvore de Decis√£o** apresentaram **desempenho inferior**, com menor F1-Score e sinais de **underfitting** (pouca capacidade de generaliza√ß√£o).

<br/>

A <b><font color="blue">**Regress√£o Log√≠stica** </font></b> √© o modelo mais indicado neste cen√°rio, combinando boa performance, f√°cil aplica√ß√£o e excelente interpretabilidade para a√ß√µes estrat√©gicas nas empresas.

#üöÄ <b><font color="blue" size="6">INTERPRETA√á√ÉO & CONCLUS√ÉO</font></b>

## üîç <b><font color="green">An√°lise de Import√¢ncia das Vari√°veis </font></b>

O gr√°fico abaixo exibe as vari√°veis mais relevantes identificadas pelo modelo **Regress√£o Log√≠stica**, treinado com dados normalizados, para prever a **evas√£o de clientes**.

As vari√°veis posicionadas no topo do gr√°fico apresentam **maior influ√™ncia nas decis√µes do modelo**, com base na magnitude dos coeficientes. Isso significa que pequenas varia√ß√µes nessas vari√°veis t√™m um impacto significativo na probabilidade de um cliente cancelar os servi√ßos.
"""

# Extrair coeficientes do modelo treinado
coeficientes = modelo_log.coef_[0]

# Import√¢ncia: valor absoluto dos coeficientes
importancias = np.abs(coeficientes)

# Criar DataFrame com vari√°veis e suas import√¢ncias
df_importancias = pd.DataFrame({
    'Vari√°vel': X_train.columns,
    'Import√¢ncia': importancias
}).sort_values(by='Import√¢ncia', ascending=False)

# Plotar gr√°fico com cor √∫nica suave, sem legenda e sem warning
plt.figure(figsize=(10, 6))
sns.barplot(
    x='Import√¢ncia',
    y='Vari√°vel',
    data=df_importancias,
    color='skyblue'
)
plt.title('Import√¢ncia das Vari√°veis - Regress√£o Log√≠stica')
plt.xlabel('Import√¢ncia (|coeficiente|)')
plt.ylabel('Vari√°veis')
plt.tight_layout()
plt.show()

"""# üß† <b><font color="blue">Conclus√£o Geral da An√°lise</font></b>

A an√°lise preditiva de evas√£o de clientes realizada neste projeto possibilitou extrair insights valiosos sobre o comportamento dos usu√°rios e os fatores que mais contribuem para o cancelamento dos servi√ßos.

<br/>

#### ‚úÖ <b><font color="blue">Principais Conclus√µes:</font></b>

- **O modelo Regress√£o Log√≠stica apresentou o melhor desempenho** entre os avaliados, alcan√ßando acur√°cia de aproximadamente **79,96%**, al√©m de melhores resultados nas m√©tricas de **Precis√£o (65,44%)**, **Recall (52,14%)** e **F1-Score (58,04%)**, essenciais para identificar corretamente clientes propensos √† evas√£o.
- O modelo Random Forest, apesar de ter boa acur√°cia (78,96%), apresentou desempenho inferior nas m√©tricas de precis√£o e recall, mostrando-se menos eficaz para o problema espec√≠fico.
- As vari√°veis com maior impacto na previs√£o incluem:
  - **Tempo de Contrato**: clientes com menor tempo tendem a evadir mais.
  - **Valor Mensal**: valores maiores influenciam negativamente a perman√™ncia.
  - **Servi√ßos adicionais** como seguran√ßa online, backup e suporte t√©cnico mostraram relev√¢ncia na modelagem.

<br/>

#### üéØ <b><font color="blue">Recomenda√ß√µes Estrat√©gicas:</font></b>

- **Desenvolver a√ß√µes de reten√ß√£o espec√≠ficas** para clientes com contratos mais curtos e maior valor mensal, grupos que apresentam maior risco de evas√£o.
- **Oferecer pacotes personalizados ou descontos progressivos** para aumentar o tempo de fideliza√ß√£o e reduzir a taxa de cancelamento.
- **Utilizar o modelo de Regress√£o Log√≠stica para monitoramento cont√≠nuo** e antecipa√ß√£o de clientes com maior probabilidade de evas√£o, possibilitando interven√ß√µes proativas.

<br/>

> üöÄ Este projeto demonstra como t√©cnicas de Machine Learning auxilia na compreens√£o de fatores essenciais para que a empresa possa **antecipar comportamentos de cancelamento** e direcionar **a√ß√µes estrat√©gicas de reten√ß√£o**, como ofertas personalizadas, melhoria no atendimento ou revis√£o de pacotes de servi√ßos.



"""